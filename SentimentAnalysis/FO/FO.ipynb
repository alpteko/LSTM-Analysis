{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_cell(object):\n",
    "\n",
    "    \"\"\"\n",
    "    LSTM cell object which takes 3 arguments for initialization.\n",
    "    input_size = Input Vector size\n",
    "    hidden_layer_size = Hidden layer size\n",
    "    target_size = Output vector size\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_layer_size, target_size):\n",
    "\n",
    "        # Initialization of given values\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.target_size = target_size\n",
    "\n",
    "\n",
    "        self.Wf = tf.Variable(tf.zeros(\n",
    "            [self.input_size, self.hidden_layer_size]))\n",
    "        self.Uf = tf.Variable(tf.zeros(\n",
    "            [self.hidden_layer_size, self.hidden_layer_size]))\n",
    "        self.bf = tf.Variable(tf.zeros([self.hidden_layer_size]))        \n",
    "\n",
    "        \n",
    "        self.Wc = tf.Variable(tf.zeros(\n",
    "            [self.input_size, self.hidden_layer_size]))\n",
    "        self.Uc = tf.Variable(tf.zeros(\n",
    "            [self.hidden_layer_size, self.hidden_layer_size]))\n",
    "        self.bc = tf.Variable(tf.zeros([self.hidden_layer_size]))\n",
    "        \n",
    "        self.Wog = tf.Variable(tf.zeros(\n",
    "            [self.input_size, self.hidden_layer_size]))\n",
    "        self.Uog = tf.Variable(tf.zeros(\n",
    "            [self.hidden_layer_size, self.hidden_layer_size]))\n",
    "        self.bog = tf.Variable(tf.zeros([self.hidden_layer_size]))\n",
    "        \n",
    "          # Weights for output layers\n",
    "        self.Wo = tf.Variable(tf.truncated_normal([self.hidden_layer_size, self.target_size], mean=0, stddev=.01))\n",
    "        self.bo = tf.Variable(tf.truncated_normal([self.target_size], mean=0, stddev=.01))\n",
    "        \n",
    "        \n",
    "        self._inputs = tf.placeholder(tf.float32,\n",
    "                                      shape=[None, None, self.input_size],\n",
    "                                      name='inputs')\n",
    "\n",
    "        # Processing inputs to work with scan function\n",
    "        self.processed_input = process_batch_input_for_RNN(self._inputs)\n",
    "\n",
    "        '''\n",
    "        Initial hidden state's shape is [1,self.hidden_layer_size]\n",
    "        In First time stamp, we are doing dot product with weights to\n",
    "        get the shape of [batch_size, self.hidden_layer_size].\n",
    "        For this dot product tensorflow use broadcasting. But during\n",
    "        Back propagation a low level error occurs.\n",
    "        So to solve the problem it was needed to initialize initial\n",
    "        hiddden state of size [batch_size, self.hidden_layer_size].\n",
    "        So here is a little hack !!!! Getting the same shaped\n",
    "        initial hidden state of zeros.\n",
    "        '''\n",
    "\n",
    "        self.initial_hidden = self._inputs[:, 0, :]\n",
    "        self.initial_hidden= tf.matmul(\n",
    "            self.initial_hidden, tf.zeros([input_size, hidden_layer_size]))\n",
    "        \n",
    "        \n",
    "        self.initial_hidden=tf.stack([self.initial_hidden,self.initial_hidden])\n",
    "    # Function for LSTM cell.\n",
    "    def Lstm(self, previous_hidden_memory_tuple, x):\n",
    "        \"\"\"\n",
    "        This function takes previous hidden state and memory tuple with input and\n",
    "        outputs current hidden state.\n",
    "        \"\"\"\n",
    "        \n",
    "        previous_hidden_state,c_prev=tf.unstack(previous_hidden_memory_tuple)\n",
    "\n",
    "        #Forget Gate\n",
    "        f = tf.sigmoid(\n",
    "            tf.matmul(x,self.Wf)+tf.matmul(previous_hidden_state,self.Uf) + self.bf)\n",
    "        \n",
    "        o  = tf.sigmoid(\n",
    "            tf.matmul(x,self.Wog)+tf.matmul(previous_hidden_state,self.Uog) + self.bog)\n",
    "          \n",
    "        c_ = tf.tanh(\n",
    "            tf.matmul(x,self.Wc)+tf.matmul(previous_hidden_state,self.Uc) + self.bc)\n",
    "        \n",
    "        #Final Memory cell\n",
    "        c = f * c_prev + (1-f) * c_\n",
    "        \n",
    "        #Current Hidden state\n",
    "        current_hidden_state = tf.tanh(c) * o\n",
    "\n",
    "\n",
    "        return tf.stack([current_hidden_state,c])\n",
    "\n",
    "    # Function for getting all hidden state.\n",
    "    def get_states(self):\n",
    "        \"\"\"\n",
    "        Iterates through time/ sequence to get all hidden state\n",
    "        \"\"\"\n",
    "\n",
    "        # Getting all hidden state throuh time\n",
    "        all_hidden_states = tf.scan(self.Lstm,\n",
    "                                    self.processed_input,\n",
    "                                    initializer=self.initial_hidden,\n",
    "                                    name='states')\n",
    "        all_hidden_states=all_hidden_states[:,0,:,:]\n",
    "        \n",
    "        return all_hidden_states\n",
    "\n",
    "    # Function to get output from a hidden layer\n",
    "    def get_output(self, hidden_state):\n",
    "        \"\"\"\n",
    "        This function takes hidden state and returns output\n",
    "        \"\"\"\n",
    "        output = tf.matmul(hidden_state, self.Wo) + self.bo\n",
    "\n",
    "        return output\n",
    "\n",
    "    # Function for getting all output layers\n",
    "    def get_outputs(self):\n",
    "        \"\"\"\n",
    "        Iterating through hidden states to get outputs for all timestamp\n",
    "        \"\"\"\n",
    "        all_hidden_states = self.get_states()\n",
    "\n",
    "        all_outputs = tf.map_fn(self.get_output, all_hidden_states)\n",
    "\n",
    "        return all_outputs\n",
    "\n",
    "\n",
    "# Function to convert batch input data to use scan ops of tensorflow.\n",
    "def process_batch_input_for_RNN(batch_input):\n",
    "    \"\"\"\n",
    "    Process tensor of size [5,3,2] to [3,5,2]\n",
    "    \"\"\"\n",
    "    batch_input_ = tf.transpose(batch_input, perm=[2, 0, 1])\n",
    "    X = tf.transpose(batch_input_)\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 64\n",
    "input_size = 50\n",
    "target_size = 2\n",
    "batchSize = 512\n",
    "maxSeqLength = 250\n",
    "ids = np.load('idsMatrix.npy')\n",
    "wordVectors = np.load('wordVectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(1,11499)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(13499,24999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(11499,13499)\n",
    "        if (num <= 12499):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "\n",
    "#data = tf.Variable(tf.zeros([batchSize, maxSeqLength, input_size]),dtype=tf.float32)\n",
    "\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "\n",
    "y = tf.placeholder(tf.float32, shape=[None, target_size],name='inputs')\n",
    "\n",
    "rnn=LSTM_cell( input_size, hidden_layer_size, target_size)\n",
    "#Getting all outputs from rnn\n",
    "outputs = rnn.get_outputs()\n",
    "last_output = outputs[-1]\n",
    "output=tf.nn.softmax(last_output)\n",
    "cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=last_output))\n",
    "train_step = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(output,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69068825 51.171875\n",
      "50.916015625\n",
      "0.69134617 51.367188\n",
      "51.7734375\n",
      "0.6806017 54.492188\n",
      "52.9169921875\n",
      "0.69265074 49.21875\n",
      "50.3466796875\n",
      "0.6927154 49.804688\n",
      "50.794921875\n",
      "0.6926296 49.804688\n",
      "50.21484375\n",
      "0.6954291 50.195312\n",
      "53.048828125\n",
      "0.6899281 51.171875\n",
      "53.1943359375\n",
      "0.69260544 50.78125\n",
      "53.87109375\n",
      "0.691569 51.757812\n",
      "53.5146484375\n",
      "0.57587343 75.58594\n",
      "74.291015625\n",
      "0.43323922 80.27344\n",
      "82.1611328125\n",
      "0.37944847 82.61719\n",
      "83.064453125\n",
      "0.36287767 85.9375\n",
      "82.9912109375\n",
      "0.37459365 82.22656\n",
      "84.224609375\n",
      "0.3617777 84.57031\n",
      "84.5693359375\n",
      "0.3734659 82.61719\n",
      "83.57421875\n",
      "0.35048693 84.375\n",
      "85.0263671875\n",
      "0.28830254 87.890625\n",
      "85.7060546875\n",
      "0.3081714 87.30469\n",
      "85.263671875\n",
      "0.32179138 85.35156\n",
      "85.505859375\n",
      "0.33240157 85.35156\n",
      "84.7470703125\n",
      "0.30365664 86.13281\n",
      "85.4609375\n",
      "0.2340285 90.42969\n",
      "85.6015625\n",
      "0.2506775 89.64844\n",
      "84.91015625\n",
      "0.26440242 88.671875\n",
      "85.4365234375\n",
      "0.24257681 91.015625\n",
      "85.2470703125\n",
      "0.30055174 90.234375\n",
      "85.630859375\n",
      "0.269355 89.0625\n",
      "85.3984375\n",
      "0.20395483 92.38281\n",
      "85.07421875\n",
      "0.2337995 90.42969\n",
      "85.078125\n",
      "0.22448042 91.60156\n",
      "85.1357421875\n",
      "0.23302259 91.21094\n",
      "85.3662109375\n",
      "0.17053738 92.578125\n",
      "84.3779296875\n",
      "0.16231918 94.53125\n",
      "83.9580078125\n",
      "0.17206252 94.33594\n",
      "84.0361328125\n",
      "0.18523203 93.94531\n",
      "84.734375\n",
      "0.1582306 95.89844\n",
      "84.1611328125\n",
      "0.11240986 95.89844\n",
      "83.7998046875\n",
      "0.12834965 96.875\n",
      "83.513671875\n",
      "0.08783273 98.046875\n",
      "84.1923828125\n",
      "0.120929755 97.07031\n",
      "83.8017578125\n",
      "0.11332287 96.67969\n",
      "83.7646484375\n",
      "0.10240898 96.28906\n",
      "83.998046875\n",
      "0.10933675 97.265625\n",
      "83.4140625\n",
      "0.09151825 96.875\n",
      "83.296875\n",
      "0.063341215 98.63281\n",
      "83.5654296875\n",
      "0.105032064 97.65625\n",
      "83.1220703125\n",
      "0.08243825 97.85156\n",
      "83.3955078125\n",
      "0.07728172 98.4375\n",
      "82.986328125\n",
      "0.09636259 97.265625\n",
      "83.4140625\n",
      "0.06899914 98.24219\n",
      "83.078125\n",
      "0.07904206 98.046875\n",
      "82.7470703125\n",
      "0.0973447 97.65625\n",
      "82.62890625\n",
      "0.053592198 98.828125\n",
      "82.6044921875\n",
      "0.0808891 98.24219\n",
      "82.322265625\n",
      "0.13967264 94.921875\n",
      "82.6630859375\n",
      "0.031405635 99.609375\n",
      "82.3505859375\n",
      "0.06297203 98.63281\n",
      "82.40625\n",
      "0.04388052 99.21875\n",
      "81.896484375\n",
      "0.068741605 98.24219\n",
      "81.904296875\n",
      "0.06830734 98.4375\n",
      "81.9443359375\n",
      "0.0845859 98.4375\n",
      "81.580078125\n",
      "0.05665414 98.63281\n",
      "81.9365234375\n",
      "0.0299342 99.02344\n",
      "82.513671875\n",
      "0.043492403 99.21875\n",
      "82.408203125\n",
      "0.025638651 99.41406\n",
      "81.5224609375\n",
      "0.10810947 97.46094\n",
      "81.970703125\n",
      "0.045777205 98.4375\n",
      "81.32421875\n",
      "0.04784048 99.02344\n",
      "82.365234375\n",
      "0.04617113 98.828125\n",
      "82.08203125\n",
      "0.04139325 99.02344\n",
      "81.9375\n",
      "0.041594155 99.21875\n",
      "82.587890625\n",
      "0.015513958 99.80469\n",
      "83.154296875\n",
      "0.022180423 99.609375\n",
      "82.0869140625\n",
      "0.057815637 98.828125\n",
      "83.0048828125\n",
      "0.05351434 99.02344\n",
      "82.4765625\n",
      "0.032182734 99.21875\n",
      "82.8515625\n",
      "0.011213453 99.80469\n",
      "82.4501953125\n",
      "0.029422889 99.609375\n",
      "81.8798828125\n",
      "0.03183526 99.609375\n",
      "81.955078125\n",
      "0.03799496 99.41406\n",
      "82.734375\n",
      "0.0085714 100.0\n",
      "82.326171875\n",
      "0.03320502 99.41406\n",
      "81.845703125\n",
      "0.023404768 99.41406\n",
      "82.3271484375\n",
      "0.031249039 99.41406\n",
      "82.6396484375\n",
      "0.029740995 99.21875\n",
      "82.490234375\n",
      "0.029544335 99.41406\n",
      "82.228515625\n",
      "0.03139907 99.41406\n",
      "82.0107421875\n",
      "0.019644652 99.609375\n",
      "82.724609375\n",
      "0.029898742 99.21875\n",
      "82.0419921875\n",
      "0.04467767 99.02344\n",
      "82.6611328125\n",
      "0.0155198565 99.609375\n",
      "82.3173828125\n",
      "0.017324634 99.80469\n",
      "81.333984375\n",
      "0.026032748 99.609375\n",
      "82.3515625\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,9001):\n",
    "    X,Y = getTrainBatch()\n",
    "    _data = sess.run(data,feed_dict={input_data:X})\n",
    "    l,acc,_ = sess.run([cross_entropy,accuracy,train_step],feed_dict={rnn._inputs:_data, y:Y}) \n",
    "    if i % 200 == 0: \n",
    "        print(l,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.0517578125\n"
     ]
    }
   ],
   "source": [
    "total_acc = 0\n",
    "for i in range(200):\n",
    "    X,Y = getTestBatch()\n",
    "    _data = sess.run(data,feed_dict={input_data:X})\n",
    "    acc = sess.run(accuracy,feed_dict={rnn._inputs:_data, y:Y}) \n",
    "    total_acc += acc\n",
    "print(total_acc/200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
