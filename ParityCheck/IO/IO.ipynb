{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_cell(object):\n",
    "\n",
    "    \"\"\"\n",
    "    LSTM cell object which takes 3 arguments for initialization.\n",
    "    input_size = Input Vector size\n",
    "    hidden_layer_size = Hidden layer size\n",
    "    target_size = Output vector size\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_layer_size, target_size):\n",
    "\n",
    "        # Initialization of given values\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.target_size = target_size\n",
    "\n",
    "\n",
    "        self.Wi = tf.Variable(tf.zeros(\n",
    "            [self.input_size, self.hidden_layer_size]))\n",
    "        self.Ui = tf.Variable(tf.zeros(\n",
    "            [self.hidden_layer_size, self.hidden_layer_size]))\n",
    "        self.bi = tf.Variable(tf.zeros([self.hidden_layer_size]))        \n",
    "\n",
    "        \n",
    "        self.Wc = tf.Variable(tf.zeros(\n",
    "            [self.input_size, self.hidden_layer_size]))\n",
    "        self.Uc = tf.Variable(tf.zeros(\n",
    "            [self.hidden_layer_size, self.hidden_layer_size]))\n",
    "        self.bc = tf.Variable(tf.zeros([self.hidden_layer_size]))\n",
    "        \n",
    "        self.Wog = tf.Variable(tf.zeros(\n",
    "            [self.input_size, self.hidden_layer_size]))\n",
    "        self.Uog = tf.Variable(tf.zeros(\n",
    "            [self.hidden_layer_size, self.hidden_layer_size]))\n",
    "        self.bog = tf.Variable(tf.zeros([self.hidden_layer_size]))\n",
    "        \n",
    "          # Weights for output layers\n",
    "        self.Wo = tf.Variable(tf.truncated_normal([self.hidden_layer_size, self.target_size], mean=0, stddev=.01))\n",
    "        self.bo = tf.Variable(tf.truncated_normal([self.target_size], mean=0, stddev=.01))\n",
    "        \n",
    "        \n",
    "        self._inputs = tf.placeholder(tf.float32,\n",
    "                                      shape=[None, None, self.input_size],\n",
    "                                      name='inputs')\n",
    "\n",
    "        # Processing inputs to work with scan function\n",
    "        self.processed_input = process_batch_input_for_RNN(self._inputs)\n",
    "\n",
    "        '''\n",
    "        Initial hidden state's shape is [1,self.hidden_layer_size]\n",
    "        In First time stamp, we are doing dot product with weights to\n",
    "        get the shape of [batch_size, self.hidden_layer_size].\n",
    "        For this dot product tensorflow use broadcasting. But during\n",
    "        Back propagation a low level error occurs.\n",
    "        So to solve the problem it was needed to initialize initial\n",
    "        hiddden state of size [batch_size, self.hidden_layer_size].\n",
    "        So here is a little hack !!!! Getting the same shaped\n",
    "        initial hidden state of zeros.\n",
    "        '''\n",
    "\n",
    "        self.initial_hidden = self._inputs[:, 0, :]\n",
    "        self.initial_hidden= tf.matmul(\n",
    "            self.initial_hidden, tf.zeros([input_size, hidden_layer_size]))\n",
    "        \n",
    "        \n",
    "        self.initial_hidden=tf.stack([self.initial_hidden,self.initial_hidden])\n",
    "    # Function for LSTM cell.\n",
    "    def Lstm(self, previous_hidden_memory_tuple, x):\n",
    "        \"\"\"\n",
    "        This function takes previous hidden state and memory tuple with input and\n",
    "        outputs current hidden state.\n",
    "        \"\"\"\n",
    "        \n",
    "        previous_hidden_state,c_prev=tf.unstack(previous_hidden_memory_tuple)\n",
    "\n",
    "        #Forget Gate\n",
    "        \n",
    "        i  = tf.sigmoid(\n",
    "            tf.matmul(x,self.Wi)+tf.matmul(previous_hidden_state,self.Ui) + self.bi)\n",
    "          \n",
    "        c_ = tf.tanh(\n",
    "            tf.matmul(x,self.Wc)+tf.matmul(previous_hidden_state,self.Uc) + self.bc)\n",
    "        \n",
    "        #Final Memory cell\n",
    "        c = c_prev + i * c_\n",
    "        \n",
    "        o = tf.sigmoid(tf.matmul(x,self.Wog)+tf.matmul(previous_hidden_state,self.Uog) + self.bog)\n",
    "            \n",
    "        #Current Hidden state\n",
    "        current_hidden_state = tf.tanh(c) * o\n",
    "\n",
    "\n",
    "        return tf.stack([current_hidden_state,c])\n",
    "\n",
    "    # Function for getting all hidden state.\n",
    "    def get_states(self):\n",
    "        \"\"\"\n",
    "        Iterates through time/ sequence to get all hidden state\n",
    "        \"\"\"\n",
    "\n",
    "        # Getting all hidden state throuh time\n",
    "        all_hidden_states = tf.scan(self.Lstm,\n",
    "                                    self.processed_input,\n",
    "                                    initializer=self.initial_hidden,\n",
    "                                    name='states')\n",
    "        all_hidden_states=all_hidden_states[:,0,:,:]\n",
    "        \n",
    "        return all_hidden_states\n",
    "\n",
    "    # Function to get output from a hidden layer\n",
    "    def get_output(self, hidden_state):\n",
    "        \"\"\"\n",
    "        This function takes hidden state and returns output\n",
    "        \"\"\"\n",
    "        output = tf.matmul(hidden_state, self.Wo) + self.bo\n",
    "\n",
    "        return output\n",
    "\n",
    "    # Function for getting all output layers\n",
    "    def get_outputs(self):\n",
    "        \"\"\"\n",
    "        Iterating through hidden states to get outputs for all timestamp\n",
    "        \"\"\"\n",
    "        all_hidden_states = self.get_states()\n",
    "\n",
    "        all_outputs = tf.map_fn(self.get_output, all_hidden_states)\n",
    "\n",
    "        return all_outputs\n",
    "\n",
    "\n",
    "# Function to convert batch input data to use scan ops of tensorflow.\n",
    "def process_batch_input_for_RNN(batch_input):\n",
    "    \"\"\"\n",
    "    Process tensor of size [5,3,2] to [3,5,2]\n",
    "    \"\"\"\n",
    "    batch_input_ = tf.transpose(batch_input, perm=[2, 0, 1])\n",
    "    X = tf.transpose(batch_input_)\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 1\n",
    "input_size = 1\n",
    "target_size = 1\n",
    "y = tf.placeholder(tf.float32, shape=[None, target_size],name='inputs')\n",
    "rnn=LSTM_cell( input_size, hidden_layer_size, target_size)\n",
    "outputs = rnn.get_outputs()\n",
    "last_output = outputs[-1]\n",
    "output=tf.nn.sigmoid(last_output)\n",
    "cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=last_output))\n",
    "lr = tf.placeholder(dtype=tf.float32,shape=[])\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(y,tf.round(output))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "k = 2\n",
    "j = 0\n",
    "lear = 0.1\n",
    "while(1):\n",
    "    j += 1\n",
    "    choose = np.random.randint(0,1401,100)\n",
    "    X=np.random.binomial(n=1,p=0.5,size=(64,k,1))\n",
    "    Y = np.sum(X,1) % 2\n",
    "    sess.run(train_step,feed_dict={rnn._inputs:X, y:Y,lr:lear})\n",
    "    X = np.random.binomial(n=1,p=0.5,size=(128,k,1))\n",
    "    Y = np.sum(X,1) % 2\n",
    "    Loss,acc = sess.run([cross_entropy,accuracy],feed_dict={rnn._inputs:X, y:Y})\n",
    "    if Loss < 0.2 - k/100:\n",
    "        print(k,acc)\n",
    "        k += 1\n",
    "        lear *= 0.9\n",
    "    if j % 100 == 1:\n",
    "        print(Loss)\n",
    "    if k == 9:\n",
    "        break\n",
    "X_test = np.random.binomial(n=1, p=0.5, size=(2**16,16,1))\n",
    "y_test = np.sum(X_test,1) % 2\n",
    "Test_accuracy=str(sess.run(accuracy,feed_dict={rnn._inputs:X_test, y:y_test}))\n",
    "print('Result:',Test_accuracy,'Iteration:',j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "X_test = np.random.binomial(n=1, p=1, size=(1,20,1))\n",
    "hs = sess.run(outputs,feed_dict={rnn._inputs:X_test})\n",
    "plt.plot(np.reshape(hs,[-1,1]))\n",
    "plt.show()\n",
    "X_test = np.random.binomial(n=1, p=1, size=(1,20,1))\n",
    "X_test[0,5:15,0] = 0\n",
    "hs = sess.run(outputs,feed_dict={rnn._inputs:X_test})\n",
    "plt.plot(np.reshape(hs,[-1,1]))\n",
    "plt.show()\n",
    "X_test = np.random.binomial(n=1, p=0, size=(1,20,1))\n",
    "X_test[0,5:15,0] = 1\n",
    "hs = sess.run(outputs,feed_dict={rnn._inputs:X_test})\n",
    "plt.plot(np.reshape(hs,[-1,1]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
