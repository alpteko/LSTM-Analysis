{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_cell(object):\n",
    "\n",
    "    \"\"\"\n",
    "    LSTM cell object which takes 3 arguments for initialization.\n",
    "    input_size = Input Vector size\n",
    "    hidden_layer_size = Hidden layer size\n",
    "    target_size = Output vector size\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_layer_size, target_size):\n",
    "\n",
    "        # Initialization of given values\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.target_size = target_size\n",
    "\n",
    "\n",
    "        self.Wf = tf.Variable(tf.zeros(\n",
    "            [self.input_size, self.hidden_layer_size]))\n",
    "        self.Uf = tf.Variable(tf.zeros(\n",
    "            [self.hidden_layer_size, self.hidden_layer_size]))\n",
    "        self.bf = tf.Variable(tf.zeros([self.hidden_layer_size]))        \n",
    "\n",
    "        \n",
    "        self.Wc = tf.Variable(tf.zeros(\n",
    "            [self.input_size, self.hidden_layer_size]))\n",
    "        self.Uc = tf.Variable(tf.zeros(\n",
    "            [self.hidden_layer_size, self.hidden_layer_size]))\n",
    "        self.bc = tf.Variable(tf.zeros([self.hidden_layer_size]))\n",
    "        \n",
    "        self.Wog = tf.Variable(tf.zeros(\n",
    "            [self.input_size, self.hidden_layer_size]))\n",
    "        self.Uog = tf.Variable(tf.zeros(\n",
    "            [self.hidden_layer_size, self.hidden_layer_size]))\n",
    "        self.bog = tf.Variable(tf.zeros([self.hidden_layer_size]))\n",
    "        \n",
    "          # Weights for output layers\n",
    "        self.Wo = tf.Variable(tf.truncated_normal([self.hidden_layer_size, self.target_size], mean=0, stddev=.01))\n",
    "        self.bo = tf.Variable(tf.truncated_normal([self.target_size], mean=0, stddev=.01))\n",
    "        \n",
    "        \n",
    "        self._inputs = tf.placeholder(tf.float32,\n",
    "                                      shape=[None, None, self.input_size],\n",
    "                                      name='inputs')\n",
    "\n",
    "        # Processing inputs to work with scan function\n",
    "        self.processed_input = process_batch_input_for_RNN(self._inputs)\n",
    "\n",
    "        '''\n",
    "        Initial hidden state's shape is [1,self.hidden_layer_size]\n",
    "        In First time stamp, we are doing dot product with weights to\n",
    "        get the shape of [batch_size, self.hidden_layer_size].\n",
    "        For this dot product tensorflow use broadcasting. But during\n",
    "        Back propagation a low level error occurs.\n",
    "        So to solve the problem it was needed to initialize initial\n",
    "        hiddden state of size [batch_size, self.hidden_layer_size].\n",
    "        So here is a little hack !!!! Getting the same shaped\n",
    "        initial hidden state of zeros.\n",
    "        '''\n",
    "\n",
    "        self.initial_hidden = self._inputs[:, 0, :]\n",
    "        self.initial_hidden= tf.matmul(\n",
    "            self.initial_hidden, tf.zeros([input_size, hidden_layer_size]))\n",
    "        \n",
    "        \n",
    "        self.initial_hidden=tf.stack([self.initial_hidden,self.initial_hidden])\n",
    "    # Function for LSTM cell.\n",
    "    def Lstm(self, previous_hidden_memory_tuple, x):\n",
    "        \"\"\"\n",
    "        This function takes previous hidden state and memory tuple with input and\n",
    "        outputs current hidden state.\n",
    "        \"\"\"\n",
    "        \n",
    "        previous_hidden_state,c_prev=tf.unstack(previous_hidden_memory_tuple)\n",
    "\n",
    "        #Forget Gate\n",
    "        f = tf.sigmoid(\n",
    "            tf.matmul(x,self.Wf)+tf.matmul(previous_hidden_state,self.Uf) + self.bf)\n",
    "        \n",
    "        o  = tf.sigmoid(\n",
    "            tf.matmul(x,self.Wog)+tf.matmul(previous_hidden_state,self.Uog) + self.bog)\n",
    "          \n",
    "        c_ = tf.tanh(\n",
    "            tf.matmul(x,self.Wc)+tf.matmul(previous_hidden_state,self.Uc) + self.bc)\n",
    "        \n",
    "        #Final Memory cell\n",
    "        c = f * c_prev + (1-f) * c_\n",
    "        \n",
    "        #Current Hidden state\n",
    "        current_hidden_state = tf.tanh(c) * o\n",
    "\n",
    "\n",
    "        return tf.stack([current_hidden_state,c])\n",
    "\n",
    "    # Function for getting all hidden state.\n",
    "    def get_states(self):\n",
    "        \"\"\"\n",
    "        Iterates through time/ sequence to get all hidden state\n",
    "        \"\"\"\n",
    "\n",
    "        # Getting all hidden state throuh time\n",
    "        all_hidden_states = tf.scan(self.Lstm,\n",
    "                                    self.processed_input,\n",
    "                                    initializer=self.initial_hidden,\n",
    "                                    name='states')\n",
    "        all_hidden_states=all_hidden_states[:,0,:,:]\n",
    "        \n",
    "        return all_hidden_states\n",
    "\n",
    "    # Function to get output from a hidden layer\n",
    "    def get_output(self, hidden_state):\n",
    "        \"\"\"\n",
    "        This function takes hidden state and returns output\n",
    "        \"\"\"\n",
    "        output = tf.matmul(hidden_state, self.Wo) + self.bo\n",
    "\n",
    "        return output\n",
    "\n",
    "    # Function for getting all output layers\n",
    "    def get_outputs(self):\n",
    "        \"\"\"\n",
    "        Iterating through hidden states to get outputs for all timestamp\n",
    "        \"\"\"\n",
    "        all_hidden_states = self.get_states()\n",
    "\n",
    "        all_outputs = tf.map_fn(self.get_output, all_hidden_states)\n",
    "\n",
    "        return all_outputs\n",
    "\n",
    "\n",
    "# Function to convert batch input data to use scan ops of tensorflow.\n",
    "def process_batch_input_for_RNN(batch_input):\n",
    "    \"\"\"\n",
    "    Process tensor of size [5,3,2] to [3,5,2]\n",
    "    \"\"\"\n",
    "    batch_input_ = tf.transpose(batch_input, perm=[2, 0, 1])\n",
    "    X = tf.transpose(batch_input_)\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 1\n",
    "input_size = 8\n",
    "target_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32, shape=[None, target_size],name='inputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing rnn object\n",
    "rnn=LSTM_cell( input_size, hidden_layer_size, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting all outputs from rnn\n",
    "outputs = rnn.get_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting final output through indexing after reversing\n",
    "#last_output = tf.reverse(outputs,[True,False,False])[0,:,:]\n",
    "last_output = outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As rnn model output the final layer through Relu activation softmax is used for final output.\n",
    "output=tf.nn.softmax(last_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the Cross Entropy loss \n",
    "cross_entropy = -tf.reduce_sum(y * tf.log(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainning with Adadelta Optimizer\n",
    "lr = tf.placeholder(dtype=tf.float32,shape=[])\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculatio of correct prediction and accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(output,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get on hot\n",
    "def get_on_hot(number):\n",
    "    on_hot=[0]*10\n",
    "    on_hot[number]=1\n",
    "    return on_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Sklearn MNIST dataset.\n",
    "digits = datasets.load_digits()\n",
    "X=digits.images\n",
    "Y_=digits.target\n",
    "\n",
    "Y=list(map(get_on_hot,Y_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Train and test Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.22, random_state=42)\n",
    "\n",
    "#Cuttting for simple iteration\n",
    "X_train=X_train[:1400]\n",
    "y_train=np.asarray(y_train[:1400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.6 226.79254\n",
      "19.0 222.27785\n",
      "19.8 219.79059\n",
      "19.400002 218.68399\n",
      "21.6 214.67447\n",
      "19.0 212.58148\n",
      "19.0 214.68277\n",
      "21.199999 211.18602\n",
      "21.8 208.652\n",
      "22.0 205.94128\n",
      "21.4 208.1855\n",
      "21.4 207.05157\n",
      "19.6 211.92621\n",
      "23.0 197.07597\n",
      "22.6 207.34712\n",
      "20.6 196.22087\n",
      "22.8 203.28552\n",
      "23.0 203.21193\n",
      "21.0 203.49696\n",
      "20.4 200.08563\n",
      "21.199999 201.23654\n",
      "20.4 206.13342\n",
      "24.6 202.9238\n",
      "22.0 204.80705\n",
      "20.800001 202.98492\n",
      "21.6 199.79118\n",
      "21.4 198.2139\n",
      "22.400002 194.52797\n",
      "23.800001 195.95369\n",
      "23.4 203.13638\n",
      "24.4 201.08017\n",
      "24.4 197.3988\n",
      "24.0 205.61848\n",
      "24.2 195.79225\n",
      "24.8 196.05472\n",
      "23.800001 199.41978\n",
      "24.0 195.83966\n",
      "24.0 206.2693\n",
      "23.6 195.58945\n",
      "24.0 189.7372\n",
      "24.6 195.74042\n",
      "24.4 197.10538\n",
      "25.2 190.47668\n",
      "25.8 191.26816\n",
      "24.8 186.6597\n",
      "23.4 185.19069\n",
      "22.6 192.5762\n",
      "23.4 194.07849\n",
      "23.6 192.1901\n",
      "24.2 202.15204\n",
      "25.0 186.3827\n",
      "26.0 194.87793\n",
      "24.2 191.63364\n",
      "24.8 192.37051\n",
      "25.2 193.98578\n",
      "25.6 194.26054\n",
      "26.0 190.42197\n",
      "27.2 193.99776\n",
      "25.6 186.25966\n",
      "21.212122\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,6000):\n",
    "    choose = np.random.randint(0,1400,100)\n",
    "    X=X_train[choose]\n",
    "    Y=y_train[choose]\n",
    "    sess.run(train_step,feed_dict={rnn._inputs:X, y:Y,lr:0.001})\n",
    "    if epoch % 100 == 0:\n",
    "        Loss=str(sess.run(cross_entropy,feed_dict={rnn._inputs:X, y:Y}))\n",
    "        Train_accuracy=str(sess.run(accuracy,feed_dict={rnn._inputs:X_train[:500], y:y_train[:500]}))\n",
    "        print(Train_accuracy,Loss)\n",
    "        if float(Loss) < 2:\n",
    "            break\n",
    "Test_accuracy=str(sess.run(accuracy,feed_dict={rnn._inputs:X_test, y:y_test}))\n",
    "print(Test_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
